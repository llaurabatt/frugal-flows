{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671156ae-3e66-4997-85ba-b18bdfaec591",
   "metadata": {},
   "source": [
    "# Example Pipeline for e401k\n",
    "\n",
    "This notebook is a proof-of-concept for generating causal samples from external samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94fec690-e143-4b90-a218-f6652bdb2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")  # go to parent dir\n",
    "# sys.path.append(\"../data/analysis/\")  # go to parent dir\n",
    "\n",
    "import jax\n",
    "import jax.random as jr\n",
    "import jax.numpy as jnp\n",
    "# jnp.set_printoptions(precision=2)\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "from rpy2.robjects.packages import SignatureTranslatedAnonymousPackage\n",
    "\n",
    "from frugal_flows.causal_flows import independent_continuous_marginal_flow, get_independent_quantiles, train_frugal_flow\n",
    "from frugal_flows.sample_outcome import sample_outcome\n",
    "from frugal_flows.sample_marginals import from_quantiles_to_marginal_cont, from_quantiles_to_marginal_discr\n",
    "from frugal_flows.train_quantile_propensity_score import train_quantile_propensity_score\n",
    "from frugal_flows.bijections import UnivariateNormalCDF\n",
    "from frugal_flows.benchmarking import FrugalFlowModel\n",
    "from frugal_flows.sample_outcome import sample_outcome\n",
    "from frugal_flows.sample_marginals import from_quantiles_to_marginal_cont, from_quantiles_to_marginal_discr\n",
    "from frugal_flows.train_quantile_propensity_score import train_quantile_propensity_score\n",
    "import torch\n",
    "from benchmarking import compare_datasets\n",
    "\n",
    "\n",
    "import data.template_causl_simulations as causl_py\n",
    "import data.analysis.validationMethods as valMethods\n",
    "import wandb\n",
    "\n",
    "# Activate automatic conversion of rpy2 objects to pandas objects\n",
    "pandas2ri.activate()\n",
    "base = importr('base')\n",
    "utils = importr('utils')\n",
    "\n",
    "# Import the R library causl\n",
    "try:\n",
    "    causl = importr('causl')\n",
    "except Exception as e:\n",
    "    package_names = ('causl')\n",
    "    utils.install_packages(StrVector(package_names))\n",
    "\n",
    "seed = 0\n",
    "N = 2000\n",
    "B = 50\n",
    "sampling_size = 1000\n",
    "keys, *subkeys = jr.split(jr.PRNGKey(seed), 20)\n",
    "\n",
    "def clean_ate(value):\n",
    "    if isinstance(value, (list, tuple, np.ndarray)):\n",
    "        return np.mean(value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a96658fe-6620-4cfa-8de7-c1e6bdff8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_hyperparam_dict = {\n",
    "    'learning_rate': 5e-4,\n",
    "    # 'learning_rate': 0.2,\n",
    "    'RQS_knots': 8,\n",
    "    'flow_layers': 3,\n",
    "    'nn_depth': 5,    \n",
    "    'nn_width': 10,\n",
    "    'max_patience': 100,\n",
    "    'max_epochs': 20000\n",
    "}\n",
    "hyperparam_dict = {\n",
    "    'learning_rate': 0.00261635,\n",
    "    'RQS_knots': 5,\n",
    "    'flow_layers': 2,\n",
    "    'nn_depth': 3,    \n",
    "    'nn_width': 34,\n",
    "    'max_patience': 100,\n",
    "    'max_epochs': 20000\n",
    "}\n",
    "causal_margin_hyperparams_dict = {\n",
    "    'RQS_knots': 4,\n",
    "    'flow_layers': 8,\n",
    "    'nn_depth': 10,    \n",
    "    'nn_width': 50,\n",
    "}\n",
    "seed=7\n",
    "\n",
    "# Load data\n",
    "e401k = pd.read_csv('../data/filtered_401k_data.csv')\n",
    "\n",
    "# Preprocess data\n",
    "outcome_col = 'net_tfa'\n",
    "treatment_col = 'e401'\n",
    "standardised_outcome_col = f'{outcome_col}_standardised'\n",
    "Y_control = e401k.loc[e401k[treatment_col]==0, outcome_col]\n",
    "Y_control_mean = Y_control.mean()\n",
    "Y_control_std = Y_control.std()\n",
    "e401k[standardised_outcome_col] = (e401k[outcome_col] - Y_control_mean) / Y_control_std\n",
    "e401k_filtered = e401k.loc[(e401k[standardised_outcome_col] > -2) & (e401k[standardised_outcome_col] < +3)]\n",
    "X = jnp.array(e401k_filtered[treatment_col].values)[:, None]\n",
    "Y = jnp.array(e401k_filtered[standardised_outcome_col].values)[:, None]\n",
    "covariate_colnames = [col for col in e401k_filtered.columns if col not in [outcome_col,standardised_outcome_col, treatment_col]]\n",
    "# ['age', 'inc', 'educ', 'fsize', 'marr', 'twoearn', 'db', 'pira', 'hown', 'p401']\n",
    "\n",
    "cont_columns = ['age', 'inc']\n",
    "disc_columns = ['educ', 'fsize', 'marr', 'twoearn', 'db', 'pira', 'hown', 'p401']\n",
    "disc_columns = cont_columns + disc_columns\n",
    "cont_columns = []\n",
    "\n",
    "for col in cont_columns:\n",
    "    mean = e401k_filtered[col].mean()\n",
    "    std = e401k_filtered[col].std()\n",
    "    e401k_filtered[col] = (e401k_filtered[col] - mean) / std\n",
    "\n",
    "Z_cont = jnp.array(e401k_filtered[cont_columns].values).astype(float)\n",
    "Z_cont = None\n",
    "# Z_disc = jnp.array(e401k_filtered[disc_columns].values)\n",
    "Z_disc = jnp.array(e401k_filtered[disc_columns].values)\n",
    "e401k_rescaled = e401k_filtered[\n",
    "    [standardised_outcome_col, treatment_col] + covariate_colnames\n",
    "]\n",
    "\n",
    "true_ATE = 1000\n",
    "benchmark_flow = FrugalFlowModel(Y=Y, X=X, Z_disc=Z_disc, Z_cont=Z_cont, confounding_copula=None)\n",
    "e401k_for_frugal_flow = e401k_rescaled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83ee06-37f9-40f6-9183-61a2b7443851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–‹                                                                                                                    | 121/20000 [02:25<6:38:42,  1.20s/it, train=-1.8997623618224506, val=-1.4560157590190013]"
     ]
    }
   ],
   "source": [
    "benchmark_flow.train_benchmark_model(\n",
    "    training_seed=jr.PRNGKey(seed),\n",
    "    marginal_hyperparam_dict=marginal_hyperparam_dict,\n",
    "    frugal_hyperparam_dict=hyperparam_dict,\n",
    "    causal_model='location_translation',\n",
    "    causal_model_args={'ate': 0, **causal_margin_hyperparams_dict},\n",
    "    prop_flow_hyperparam_dict=causal_margin_hyperparams_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acf803f-613a-464c-a2db-ae61eadeaacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f27b4b-2005-4491-8d50-1529727b4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_outcome(x, mean, std):\n",
    "    return x * std + mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3fe7f3-e6b2-4942-8912-807a4ad07966",
   "metadata": {},
   "source": [
    "### Unconfounded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a91e4d-4135-49af-a98d-380400b9dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_df = benchmark_flow.generate_samples(\n",
    "    key=jr.PRNGKey(10*seed),\n",
    "    sampling_size=6000,\n",
    "    copula_param=0.,\n",
    "    outcome_causal_model='location_translation',\n",
    "    outcome_causal_args={'ate': true_ATE / Y_control_std},\n",
    "    with_confounding=True\n",
    ")\n",
    "sim_data_df.columns = e401k_rescaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23cd51c-01a5-4981-8a60-2158487f3bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.25)\n",
    "fig,ax = plt.subplots(ncols=2,figsize=(18,6))\n",
    "sns.heatmap(e401k_rescaled[[standardised_outcome_col] + covariate_colnames].corr(),ax=ax[0],square=True)\n",
    "ax[0].set_title('Observed e401k Data')\n",
    "sns.heatmap(sim_data_df[[standardised_outcome_col] + covariate_colnames].corr(),ax=ax[1],square=True)\n",
    "ax[1].set_title('Generated e401k Data')\n",
    "# fig.savefig('Lalonde_NSW_0_1000_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39ea61-f6a6-4cb3-894d-536c5c9962af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphas = [0.1, 0.5, 1.0, 2.0]\n",
    "# k = 3\n",
    "# compare_datasets(e401k_rescaled.sample(2000).values, sim_data_df.sample(2000).values, alphas=alphas, k=3, n_permutations=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b464220e-7d97-4d63-b496-8e5c5c006814",
   "metadata": {},
   "source": [
    "# With Credence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf04ebf-0c40-4e60-a5f4-7ff815865925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import pytorch_lightning as pl\n",
    "import tqdm\n",
    "sns.set()\n",
    "import os\n",
    "# os.chdir('/Users/harshparikh/Documents/GitHub/credence-to-causal-estimation/credence-v2/src/')\n",
    "os.chdir('./credence-to-causal-estimation/credence-v2/src/')\n",
    "import credence\n",
    "import autoencoder\n",
    "import importlib\n",
    "importlib.reload(autoencoder)\n",
    "importlib.reload(credence)\n",
    "# os.chdir('/Users/harshparikh/Documents/GitHub/credence-to-causal-estimation/notebooks/')\n",
    "os.chdir('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ee0e3-0fcb-4a7a-9de7-c6bb057bda59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "e401k = pd.read_csv('../data/filtered_401k_data.csv')\n",
    "\n",
    "# Preprocess data\n",
    "outcome_col = 'net_tfa'\n",
    "treatment_col = 'e401'\n",
    "standardised_outcome_col = f'{outcome_col}_standardised'\n",
    "Y_control = e401k.loc[e401k[treatment_col]==0, outcome_col]\n",
    "Y_control_mean = Y_control.mean()\n",
    "Y_control_std = Y_control.std()\n",
    "e401k[standardised_outcome_col] = (e401k[outcome_col] - Y_control_mean) / Y_control_std\n",
    "e401k_filtered = e401k.loc[(e401k[standardised_outcome_col] > -2) & (e401k[standardised_outcome_col] < +3)]\n",
    "X = jnp.array(e401k_filtered[treatment_col].values)[:, None]\n",
    "Y = jnp.array(e401k_filtered[standardised_outcome_col].values)[:, None]\n",
    "covariate_colnames = [col for col in e401k_filtered.columns if col not in [outcome_col,standardised_outcome_col, treatment_col]]\n",
    "# ['age', 'inc', 'educ', 'fsize', 'marr', 'twoearn', 'db', 'pira', 'hown', 'p401']\n",
    "\n",
    "cont_columns = ['age', 'inc', 'educ']\n",
    "disc_columns = ['fsize', 'marr', 'twoearn', 'db', 'pira', 'hown', 'p401']\n",
    "# disc_columns = cont_columns + disc_columns\n",
    "# cont_columns = []\n",
    "\n",
    "for col in cont_columns:\n",
    "    mean = e401k_filtered[col].mean()\n",
    "    std = e401k_filtered[col].std()\n",
    "    e401k_filtered[col] = (e401k_filtered[col] - mean) / std\n",
    "\n",
    "Z_cont = jnp.array(e401k_filtered[cont_columns].values).astype(float)\n",
    "Z_cont = None\n",
    "# Z_disc = jnp.array(e401k_filtered[disc_columns].values)\n",
    "Z_disc = jnp.array(e401k_filtered[disc_columns].values)\n",
    "e401k_rescaled = e401k_filtered[\n",
    "    [standardised_outcome_col, treatment_col] + covariate_colnames\n",
    "]\n",
    "\n",
    "true_ATE = 1000\n",
    "benchmark_flow = FrugalFlowModel(Y=Y, X=X, Z_disc=Z_disc, Z_cont=Z_cont, confounding_copula=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291be82-bf15-4b4d-acc6-efba7d63d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_columns = ['age', 'inc', 'educ']\n",
    "disc_columns = ['fsize', 'marr', 'twoearn', 'db', 'pira', 'hown', 'p401']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9952d2-3c8c-4056-8f0c-6722088df031",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "v1000 = credence.Credence(\n",
    "    data=e401k_rescaled, # dataframe \n",
    "    post_treatment_var=[standardised_outcome_col], # list of post treatment variables\n",
    "    treatment_var=[treatment_col], # list of treatment variable(s)\n",
    "    categorical_var=[treatment_col] + disc_columns, # list of variables which are categorical\n",
    "    numerical_var= [standardised_outcome_col] + cont_columns # list of variables which are numerical\n",
    ")\n",
    "gen = v1000.fit(effect_rigidity=1000,bias_rigidity=1000,kld_rigidity=0.01,max_epochs=250);\n",
    "# v.trainer_treat.save_checkpoint(\"e401k_treat_0.ckpt\");\n",
    "# v.trainer_pre.save_checkpoint(\"e401k_pre_0.ckpt\");\n",
    "# v.trainer_post.save_checkpoint(\"e401k_post_0.ckpt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c7828-f48e-4f57-bfe2-5ad6c9ca0ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_credence1000, sim_data_credence_prime1000 = v1000.sample(data=e401k_rescaled)\n",
    "sim_data_credence1000[standardised_outcome_col] = sim_data_credence1000['Y0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a009ae-bbd5-4a0d-bb8a-8c07311cfa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "v5000 = credence.Credence(\n",
    "    data=e401k_rescaled, # dataframe \n",
    "    post_treatment_var=[standardised_outcome_col], # list of post treatment variables\n",
    "    treatment_var=[treatment_col], # list of treatment variable(s)\n",
    "    categorical_var=[treatment_col] + disc_columns, # list of variables which are categorical\n",
    "    numerical_var= [standardised_outcome_col] + cont_columns # list of variables which are numerical\n",
    ")\n",
    "gen = v5000.fit(effect_rigidity=1000,bias_rigidity=5000,kld_rigidity=0.01,max_epochs=250);\n",
    "# v.trainer_treat.save_checkpoint(\"e401k_treat_0.ckpt\");\n",
    "# v.trainer_pre.save_checkpoint(\"e401k_pre_0.ckpt\");\n",
    "# v.trainer_post.save_checkpoint(\"e401k_post_0.ckpt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa391011-c51d-41f0-b0a5-120f6c4cb68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_credence5000, sim_data_credence_prime = v5000.sample(data=e401k_rescaled)\n",
    "sim_data_credence5000[standardised_outcome_col] = sim_data_credence5000['Y0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108875e2-ad17-490e-941d-18852295d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(font_scale=1.25)\n",
    "# fig,ax = plt.subplots(ncols=2,figsize=(18,6))\n",
    "# sns.heatmap(e401k_rescaled[[standardised_outcome_col] + covariate_colnames].corr(),ax=ax[0],square=True)\n",
    "# ax[0].set_title('Observed NSW Data')\n",
    "# sns.heatmap(sim_data_credence[[standardised_outcome_col] + covariate_colnames].corr(),ax=ax[1],square=True)\n",
    "# ax[1].set_title('Generated NSW Data')\n",
    "# fig.savefig('Lalonde_NSW_0_1000_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae37bdd-85d5-49e4-8b28-344356a3b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(font_scale=1.25)\n",
    "\n",
    "# Create the subplots\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(22, 6), gridspec_kw={'width_ratios': [1, 1, 1]})\n",
    "\n",
    "# Define the data for each heatmap\n",
    "data_list = [\n",
    "    e401k_rescaled[covariate_colnames + [standardised_outcome_col]].corr(),\n",
    "    sim_data_df[covariate_colnames + [standardised_outcome_col]].corr(),\n",
    "    sim_data_credence[covariate_colnames + [standardised_outcome_col]].corr(),\n",
    "]\n",
    "\n",
    "# Titles for each subplot\n",
    "titles = [\n",
    "    'Observed NSW Data',\n",
    "    'Frugal Flows',\n",
    "    'CREDENCE (bias = 5000)',\n",
    "]\n",
    "\n",
    "# Find the common vmin and vmax for consistent color scaling\n",
    "vmin = min(d.min().min() for d in data_list)\n",
    "vmax = max(d.max().max() for d in data_list)\n",
    "\n",
    "# Create a colorbar axis\n",
    "cbar_ax = fig.add_axes([0.92, 0.3, 0.02, 0.4])\n",
    "\n",
    "# Plot each heatmap\n",
    "for i, data in enumerate(data_list):\n",
    "    sns.heatmap(data, ax=ax[i], square=True, vmin=vmin, vmax=vmax, cbar=(i == 2), cbar_ax=cbar_ax if i == 2 else None,\n",
    "                yticklabels=(i == 0))\n",
    "    ax[i].set_title(titles[i])\n",
    "    if i != 0:\n",
    "        ax[i].set_yticklabels([])\n",
    "\n",
    "# Adjust layout\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig('e401k_0_1000_0.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5743072-ecce-44c2-9721-0fad84ec67ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(font_scale=1.25)\n",
    "\n",
    "# Create the subplots\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(22, 6), gridspec_kw={'width_ratios': [1, 1, 1, 1]})\n",
    "\n",
    "# Define the data for each heatmap\n",
    "data_list = [\n",
    "    e401k_rescaled[covariate_colnames + [standardised_outcome_col]].corr(),\n",
    "    sim_data_df[covariate_colnames + [standardised_outcome_col]].corr(),\n",
    "    sim_data_credence5000[covariate_colnames + [standardised_outcome_col]].corr(),\n",
    "    sim_data_credence1000[covariate_colnames + [standardised_outcome_col]].corr()\n",
    "]\n",
    "\n",
    "# Titles for each subplot\n",
    "titles = [\n",
    "    'Observed NSW Data',\n",
    "    'Frugal Flows',\n",
    "    'CREDENCE (bias = 5000) -- Original',\n",
    "    'CREDENCE (bias = 1000)'\n",
    "]\n",
    "\n",
    "# Find the common vmin and vmax for consistent color scaling\n",
    "vmin = min(d.min().min() for d in data_list)\n",
    "vmax = max(d.max().max() for d in data_list)\n",
    "\n",
    "# Create a colorbar axis\n",
    "cbar_ax = fig.add_axes([0.92, 0.3, 0.02, 0.4])\n",
    "\n",
    "# Plot each heatmap\n",
    "for i, data in enumerate(data_list):\n",
    "    sns.heatmap(data, ax=ax[i], square=True, vmin=vmin, vmax=vmax, cbar=(i == 3), cbar_ax=cbar_ax if i == 3 else None,\n",
    "                yticklabels=(i == 0))\n",
    "    ax[i].set_title(titles[i])\n",
    "    if i != 0:\n",
    "        ax[i].set_yticklabels([])\n",
    "\n",
    "# Adjust layout\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "# fig.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig('e401k_0_1000_0.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd03c9-0105-41b2-856f-b3d8a717d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.1]#, 0.25, 0.5, 0.75, 1.0, 1.5, 2.0]\n",
    "k = 3\n",
    "# cols = ['age', 'black', 'education']\n",
    "cols = covariate_colnames + [standardised_outcome_col]\n",
    "n = 600\n",
    "metric_dict = {}\n",
    "metric_dict['Frugal Flows'] = compare_datasets(e401k_for_frugal_flow.loc[:, cols].sample(n).values, sim_data_df.loc[:, cols].sample(n).values, alphas=alphas, k=3, n_permutations=1000)\n",
    "metric_dict['CREDENCE (Bias = 1000)'] = compare_datasets(e401k_rescaled.loc[:, cols].sample(n).values, sim_data_credence1000.loc[:, cols].sample(n).values, alphas=alphas, k=3, n_permutations=1000)\n",
    "metric_dict['CREDENCE (Bias = 5000)'] = compare_datasets(e401k_rescaled.loc[:, cols].sample(n).values, sim_data_credence5000.loc[:, cols].sample(n).values, alphas=alphas, k=3, n_permutations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb4a93-3372-4ab9-aecb-4738664a1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(metric_dict, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f7ed5c-0344-4605-8ef2-99de50ed513f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
