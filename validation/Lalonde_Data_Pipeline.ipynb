{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671156ae-3e66-4997-85ba-b18bdfaec591",
   "metadata": {},
   "source": [
    "# Example Pipeline for Lalonde\n",
    "\n",
    "This notebook is a proof-of-concept for generating causal samples from external samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94fec690-e143-4b90-a218-f6652bdb2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")  # go to parent dir\n",
    "# sys.path.append(\"../data/analysis/\")  # go to parent dir\n",
    "\n",
    "import jax\n",
    "import jax.random as jr\n",
    "import jax.numpy as jnp\n",
    "# jnp.set_printoptions(precision=2)\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "from frugal_flows.causal_flows import independent_continuous_marginal_flow, get_independent_quantiles, train_frugal_flow\n",
    "from frugal_flows.sample_outcome import sample_outcome\n",
    "from frugal_flows.sample_marginals import from_quantiles_to_marginal_cont, from_quantiles_to_marginal_discr\n",
    "from frugal_flows.train_quantile_propensity_score import train_quantile_propensity_score\n",
    "from frugal_flows.bijections import UnivariateNormalCDF\n",
    "from frugal_flows.benchmarking import FrugalFlowModel\n",
    "from frugal_flows.sample_outcome import sample_outcome\n",
    "from frugal_flows.sample_marginals import from_quantiles_to_marginal_cont, from_quantiles_to_marginal_discr\n",
    "from frugal_flows.train_quantile_propensity_score import train_quantile_propensity_score\n",
    "\n",
    "\n",
    "import data.template_causl_simulations as causl_py\n",
    "import data.analysis.validationMethods as valMethods\n",
    "import wandb\n",
    "\n",
    "# Activate automatic conversion of rpy2 objects to pandas objects\n",
    "pandas2ri.activate()\n",
    "base = importr('base')\n",
    "utils = importr('utils')\n",
    "\n",
    "# Import the R library causl\n",
    "try:\n",
    "    causl = importr('causl')\n",
    "except Exception as e:\n",
    "    package_names = ('causl')\n",
    "    utils.install_packages(StrVector(package_names))\n",
    "\n",
    "seed = 0\n",
    "N = 2000\n",
    "B = 50\n",
    "sampling_size = 1000\n",
    "keys, *subkeys = jr.split(jr.PRNGKey(seed), 20)\n",
    "\n",
    "def clean_ate(value):\n",
    "    if isinstance(value, (list, tuple, np.ndarray)):\n",
    "        return np.mean(value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a96658fe-6620-4cfa-8de7-c1e6bdff8930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%| | 279/20000 [00:49<58:12,  5.65it/s, train=-0.8595963758529483, val=0.8211046335156056 (Max patience \n",
      "  1%|▏             | 221/20000 [00:16<23:59, 13.74it/s, train=-0.6485195269727503, val=-0.5418439828002886]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m true_ATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     36\u001b[0m benchmark_flow \u001b[38;5;241m=\u001b[39m FrugalFlowModel(Y\u001b[38;5;241m=\u001b[39mY, X\u001b[38;5;241m=\u001b[39mX, Z_disc\u001b[38;5;241m=\u001b[39mZ_disc, Z_cont\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, confounding_copula\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mbenchmark_flow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_benchmark_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPRNGKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmarginal_hyperparam_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparam_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrugal_hyperparam_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparam_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocation_translation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_model_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcausal_margin_hyperparams_dict\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprop_flow_hyperparam_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparam_dict\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-danielmanela@gmail.com/My Drive/work/deep-copula-frugal/validation/../frugal_flows/benchmarking.py:72\u001b[0m, in \u001b[0;36mFrugalFlowModel.train_benchmark_model\u001b[0;34m(self, training_seed, marginal_hyperparam_dict, frugal_hyperparam_dict, causal_model, causal_model_args, prop_flow_hyperparam_dict)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_marginal_cdfs(training_seeds[\u001b[38;5;241m0\u001b[39m], marginal_hyperparam_dict)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_frugal_flow(training_seeds[\u001b[38;5;241m1\u001b[39m], frugal_hyperparam_dict, causal_model, causal_model_args)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_propensity_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_seeds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprop_flow_hyperparam_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-danielmanela@gmail.com/My Drive/work/deep-copula-frugal/validation/../frugal_flows/benchmarking.py:110\u001b[0m, in \u001b[0;36mFrugalFlowModel.train_propensity_flow\u001b[0;34m(self, key, hyperparam_dict)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     condition \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mhstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mZ_disc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mZ_cont])\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprop_flow, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_quantile_propensity_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhyperparam_dict\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m prop_flow_cdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprop_flow\u001b[38;5;241m.\u001b[39mbijection\u001b[38;5;241m.\u001b[39mtransform\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvmap_prop_flow \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(prop_flow_cdf, in_axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,))\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-danielmanela@gmail.com/My Drive/work/deep-copula-frugal/validation/../frugal_flows/train_quantile_propensity_score.py:81\u001b[0m, in \u001b[0;36mtrain_quantile_propensity_score\u001b[0;34m(key, x, condition, optimizer, RQS_knots, nn_depth, nn_width, flow_layers, show_progress, learning_rate, max_epochs, max_patience, batch_size, return_x_quantiles)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m     80\u001b[0m key, subkey \u001b[38;5;241m=\u001b[39m jr\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[0;32m---> 81\u001b[0m flow, losses \u001b[38;5;241m=\u001b[39m \u001b[43mfit_to_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mu_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_patience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_x_quantiles:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flow, losses, u_x\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-danielmanela@gmail.com/My Drive/work/deep-copula-frugal/flowjax/flowjax/train/data_fit.py:100\u001b[0m, in \u001b[0;36mfit_to_data\u001b[0;34m(key, dist, x, condition, loss_fn, max_epochs, max_patience, batch_size, val_prop, learning_rate, optimizer, return_best, show_progress)\u001b[0m\n\u001b[1;32m     98\u001b[0m batch_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mget_batches(train_data, batch_size), strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 100\u001b[0m     params, opt_state, loss_i \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     batch_losses\u001b[38;5;241m.\u001b[39mappend(loss_i)\n\u001b[1;32m    109\u001b[0m losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m(batch_losses) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_losses))\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/micromamba/envs/deep-frugal/lib/python3.11/site-packages/equinox/_module.py:910\u001b[0m, in \u001b[0;36m_unflatten_module\u001b[0;34m(cls, aux, dynamic_field_values)\u001b[0m\n\u001b[1;32m    900\u001b[0m     aux \u001b[38;5;241m=\u001b[39m _FlattenedData(\n\u001b[1;32m    901\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(dynamic_field_names),\n\u001b[1;32m    902\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(static_field_names),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(wrapper_field_values),\n\u001b[1;32m    906\u001b[0m     )\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(dynamic_field_values), aux\n\u001b[0;32m--> 910\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unflatten_module\u001b[39m(\u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m], aux: _FlattenedData, dynamic_field_values):\n\u001b[1;32m    911\u001b[0m     \u001b[38;5;66;03m# This doesn't go via `__init__`. A user may have done something nontrivial there,\u001b[39;00m\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;66;03m# and the field values may be dummy values as used in various places throughout JAX.\u001b[39;00m\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;66;03m# See also\u001b[39;00m\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;66;03m# https://jax.readthedocs.io/en/latest/pytrees.html#custom-pytrees-and-initialization,\u001b[39;00m\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;66;03m# which was (I believe) inspired by Equinox's approach here.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(aux\u001b[38;5;241m.\u001b[39mdynamic_field_names, dynamic_field_values):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperparam_dict = {\n",
    "    'learning_rate': 0.006335,\n",
    "    'RQS_knots': 4,\n",
    "    'flow_layers': 9,\n",
    "    'nn_depth': 10,    \n",
    "    'nn_width': 50,\n",
    "    'max_patience': 200,\n",
    "    'max_epochs': 20000\n",
    "}\n",
    "causal_margin_hyperparams_dict = {\n",
    "    'RQS_knots': 4,\n",
    "    'flow_layers': 8,\n",
    "    'nn_depth': 10,    \n",
    "    'nn_width': 50,\n",
    "}\n",
    "seed=1\n",
    "\n",
    "lalonde = pd.read_csv('../data/filtered_lalonde_dataset.csv')\n",
    "lalonde = lalonde\n",
    "\n",
    "outcome_col = 're78'\n",
    "treatment_col = 'treatment'\n",
    "standardised_outcome_col = f'{outcome_col}_standardised'\n",
    "Y_control = lalonde.loc[lalonde[treatment_col]==0, outcome_col]\n",
    "Y_control_mean = Y_control.mean()\n",
    "Y_control_std = Y_control.std()\n",
    "lalonde[standardised_outcome_col] = (lalonde[outcome_col] - Y_control_mean) / Y_control_std\n",
    "\n",
    "X = jnp.array(lalonde[treatment_col].values)[:, None]\n",
    "Y = jnp.array(lalonde[standardised_outcome_col].values)[:, None]\n",
    "\n",
    "covariate_colnames = ['treatment', 'age', 'education', 'black', 'hispanic', 'married', 'nodegree']\n",
    "Z_disc = jnp.array(lalonde[covariate_colnames].values)\n",
    "\n",
    "true_ATE = 1000\n",
    "benchmark_flow = FrugalFlowModel(Y=Y, X=X, Z_disc=Z_disc, Z_cont=None, confounding_copula=None)\n",
    "benchmark_flow.train_benchmark_model(\n",
    "    training_seed=jr.PRNGKey(seed),\n",
    "    marginal_hyperparam_dict=hyperparam_dict,\n",
    "    frugal_hyperparam_dict=hyperparam_dict,\n",
    "    causal_model='location_translation',\n",
    "    causal_model_args={'ate': 0, **causal_margin_hyperparams_dict},\n",
    "    prop_flow_hyperparam_dict=hyperparam_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f27b4b-2005-4491-8d50-1529727b4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_outcome(x, mean, std):\n",
    "    return x * std + mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3fe7f3-e6b2-4942-8912-807a4ad07966",
   "metadata": {},
   "source": [
    "### Unconfounded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198d01ae-2246-4f76-9d0b-80bed287bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_confounded_results_list = []\n",
    "for seed in range(B):\n",
    "    print(f\"Run {seed+1} / {B}\")\n",
    "    sim_data_df = benchmark_flow.generate_samples(\n",
    "        key=jr.PRNGKey(10*seed),\n",
    "        sampling_size=sampling_size,\n",
    "        copula_param=0.,\n",
    "        outcome_causal_model='location_translation',\n",
    "        outcome_causal_args={'ate': true_ATE / Y_control_std},\n",
    "        with_confounding=False\n",
    "    )\n",
    "    sim_data_df['Y'] = sim_data_df['Y'].apply(lambda x: rescale_outcome(x, Y_control_mean, Y_control_std))\n",
    "    with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "        model_fits = valMethods.run_model_fits('Y', 'X', sim_data_df, sample_frac=1, repeats=1, replace=True)\n",
    "    ate_reslts = pd.concat([\n",
    "        model_fits['nonbootstrap_results'][['method', 'ate']], \n",
    "        model_fits['bootstrap_results'][['method', 'ate']]\n",
    "    ])\n",
    "    non_confounded_results_list.append(ate_reslts)\n",
    "non_confounded_model_fit_results = pd.concat(non_confounded_results_list)\n",
    "non_confounded_model_fit_results = non_confounded_model_fit_results.loc[\n",
    "    ~non_confounded_model_fit_results['method'].isin(['Gradient Boosting Trees DML', 'Doubly Robust (Linear)'])\n",
    "]\n",
    "non_confounded_model_fit_results['ate'] = non_confounded_model_fit_results['ate'].apply(clean_ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072ec25c-b320-44a7-9e4b-e637d2d6417f",
   "metadata": {},
   "source": [
    "### Confounded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ee374-828a-4e74-9978-5e5292b30572",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounded_results_list = []\n",
    "for seed in range(B):\n",
    "    print(f\"Run {seed+1} / {B}\")\n",
    "    sim_data_df = benchmark_flow.generate_samples(\n",
    "        key=jr.PRNGKey(10*seed),\n",
    "        sampling_size=sampling_size,\n",
    "        copula_param=0.,\n",
    "        outcome_causal_model='location_translation',\n",
    "        outcome_causal_args={'ate': true_ATE / Y_control_std},\n",
    "        with_confounding=True\n",
    "    )\n",
    "    sim_data_df['Y'] = sim_data_df['Y'].apply(lambda x: rescale_outcome(x, Y_control_mean, Y_control_std))    \n",
    "    with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "        model_fits = valMethods.run_model_fits('Y', 'X', sim_data_df, sample_frac=1, repeats=1, replace=True)\n",
    "    ate_reslts = pd.concat([\n",
    "        model_fits['nonbootstrap_results'][['method', 'ate']], \n",
    "        model_fits['bootstrap_results'][['method', 'ate']]\n",
    "    ])\n",
    "    confounded_results_list.append(ate_reslts)\n",
    "confounded_model_fit_results = pd.concat(confounded_results_list)\n",
    "confounded_model_fit_results = confounded_model_fit_results.loc[\n",
    "    ~confounded_model_fit_results['method'].isin(['Gradient Boosting Trees DML', 'Doubly Robust (Linear)'])\n",
    "]\n",
    "confounded_model_fit_results['ate'] = confounded_model_fit_results['ate'].apply(clean_ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a152104-1a86-40cc-a277-97c09b3e553f",
   "metadata": {},
   "source": [
    "### Hidden Confounded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be9f569-3936-449d-bbb0-84b87fde60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_confounded_results_list = []\n",
    "for seed in range(B):\n",
    "    print(f\"Run {seed+1} / {B}\")\n",
    "    sim_data_df = benchmark_flow.generate_samples(\n",
    "        key=jr.PRNGKey(10*seed),\n",
    "        sampling_size=sampling_size,\n",
    "        copula_param=0.5,\n",
    "        outcome_causal_model='location_translation',\n",
    "        outcome_causal_args={'ate': true_ATE / Y_control_std},\n",
    "        with_confounding=True\n",
    "    )\n",
    "    sim_data_df['Y'] = sim_data_df['Y'].apply(lambda x: rescale_outcome(x, Y_control_mean, Y_control_std))    \n",
    "    with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "        model_fits = valMethods.run_model_fits('Y', 'X', sim_data_df, sample_frac=1, repeats=1, replace=True)\n",
    "    ate_reslts = pd.concat([\n",
    "        model_fits['nonbootstrap_results'][['method', 'ate']], \n",
    "        model_fits['bootstrap_results'][['method', 'ate']]\n",
    "    ])\n",
    "    hidden_confounded_results_list.append(ate_reslts)\n",
    "hidden_confounded_model_fit_results = pd.concat(hidden_confounded_results_list)\n",
    "hidden_confounded_model_fit_results = hidden_confounded_model_fit_results.loc[\n",
    "    ~hidden_confounded_model_fit_results['method'].isin(['Gradient Boosting Trees DML', 'Doubly Robust (Linear)'])\n",
    "]\n",
    "hidden_confounded_model_fit_results['ate'] = hidden_confounded_model_fit_results['ate'].apply(clean_ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ea816a-a7cf-4596-82f0-8ad1dccfa232",
   "metadata": {},
   "source": [
    "## Some demo plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595b056-8e33-4c42-9126-9fb807b861b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['text.usetex'] = True\n",
    "\n",
    "def clean_ate(value):\n",
    "    if isinstance(value, (list, tuple, np.ndarray)):\n",
    "        return np.mean(value)\n",
    "    return value\n",
    "\n",
    "# Apply the cleaning function to the data\n",
    "df1 = non_confounded_model_fit_results.copy()\n",
    "df2 = confounded_model_fit_results.copy()\n",
    "df3 = hidden_confounded_model_fit_results.copy()\n",
    "\n",
    "# Group data by method\n",
    "grouped_df1 = df1.groupby('method')['ate'].apply(list).reset_index()\n",
    "grouped_df2 = df2.groupby('method')['ate'].apply(list).reset_index()\n",
    "grouped_df3 = df3.groupby('method')['ate'].apply(list).reset_index()\n",
    "\n",
    "# Plot the box and whisker diagrams side by side\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(14, 6), sharey=True)\n",
    "\n",
    "# Define a common font size\n",
    "font_size = 16\n",
    "title_fontsize = font_size + 2\n",
    "\n",
    "# Create the boxplot for the first dataset\n",
    "axes[0].boxplot(grouped_df1['ate'], vert=False, patch_artist=True, labels=grouped_df1['method'])\n",
    "axes[0].axvline(x=true_ATE, color='red', linestyle='--')\n",
    "axes[0].set_xlabel('ATE', fontsize=font_size)\n",
    "axes[0].set_title('No Confounding', fontsize=title_fontsize)\n",
    "axes[0].tick_params(axis='both', which='major', labelsize=font_size)\n",
    "axes[0].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Create the boxplot for the second dataset\n",
    "axes[1].boxplot(grouped_df2['ate'], vert=False, patch_artist=True, labels=grouped_df2['method'])\n",
    "axes[1].axvline(x=true_ATE, color='red', linestyle='--')\n",
    "axes[1].set_xlabel('ATE', fontsize=font_size)\n",
    "axes[1].set_title(r'With Real-World Confounding', fontsize=title_fontsize)\n",
    "axes[1].tick_params(axis='both', which='major', labelsize=font_size)\n",
    "axes[1].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Create the boxplot for the third dataset\n",
    "axes[2].boxplot(grouped_df3['ate'], vert=False, patch_artist=True, labels=grouped_df3['method'])\n",
    "axes[2].axvline(x=true_ATE, color='red', linestyle='--')\n",
    "axes[2].set_xlabel('ATE', fontsize=font_size)\n",
    "axes[2].set_title(r'With Hidden Confounding $\\rho=0.5$', fontsize=title_fontsize)\n",
    "axes[2].tick_params(axis='both', which='major', labelsize=font_size)\n",
    "axes[2].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF\n",
    "plt.savefig('lalonde_box_and_whisker_plots.pdf', format='pdf')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feda6a1-3543-4258-8e3c-d8f422a0508f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
