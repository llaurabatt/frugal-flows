{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671156ae-3e66-4997-85ba-b18bdfaec591",
   "metadata": {},
   "source": [
    "# Example Pipeline for Lalonde\n",
    "\n",
    "This notebook is a proof-of-concept for generating causal samples from external samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94fec690-e143-4b90-a218-f6652bdb2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")  # go to parent dir\n",
    "# sys.path.append(\"../data/analysis/\")  # go to parent dir\n",
    "\n",
    "import jax\n",
    "import jax.random as jr\n",
    "import jax.numpy as jnp\n",
    "# jnp.set_printoptions(precision=2)\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "from rpy2.robjects.packages import SignatureTranslatedAnonymousPackage\n",
    "\n",
    "from frugal_flows.causal_flows import independent_continuous_marginal_flow, get_independent_quantiles, train_frugal_flow\n",
    "from frugal_flows.sample_outcome import sample_outcome\n",
    "from frugal_flows.sample_marginals import from_quantiles_to_marginal_cont, from_quantiles_to_marginal_discr\n",
    "from frugal_flows.train_quantile_propensity_score import train_quantile_propensity_score\n",
    "from frugal_flows.bijections import UnivariateNormalCDF\n",
    "from frugal_flows.benchmarking import FrugalFlowModel\n",
    "from frugal_flows.sample_outcome import sample_outcome\n",
    "from frugal_flows.sample_marginals import from_quantiles_to_marginal_cont, from_quantiles_to_marginal_discr\n",
    "from frugal_flows.train_quantile_propensity_score import train_quantile_propensity_score\n",
    "\n",
    "\n",
    "# import data.template_causl_simulations as causl_py\n",
    "import validationMethods as valMethods\n",
    "import wandb\n",
    "\n",
    "# Activate automatic conversion of rpy2 objects to pandas objects\n",
    "pandas2ri.activate()\n",
    "base = importr('base')\n",
    "utils = importr('utils')\n",
    "\n",
    "# Import the R library causl\n",
    "try:\n",
    "    causl = importr('causl')\n",
    "except Exception as e:\n",
    "    package_names = ('causl')\n",
    "    utils.install_packages(StrVector(package_names))\n",
    "\n",
    "seed = 0\n",
    "N = 2000\n",
    "B = 10\n",
    "sampling_size = 1000\n",
    "keys, *subkeys = jr.split(jr.PRNGKey(seed), 20)\n",
    "\n",
    "def clean_ate(value):\n",
    "    if isinstance(value, (list, tuple, np.ndarray)):\n",
    "        return np.mean(value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96658fe-6620-4cfa-8de7-c1e6bdff8930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%| | 217/20000 [04:21<6:21:33,  1.16s/it, train=-1.9913531193510545, val=-1.423140"
     ]
    }
   ],
   "source": [
    "hyperparam_dict = {\n",
    "    'learning_rate': 0.00261635,\n",
    "    'RQS_knots': 5,\n",
    "    'flow_layers': 2,\n",
    "    'nn_depth': 3,    \n",
    "    'nn_width': 34,\n",
    "    'max_patience': 200,\n",
    "    'max_epochs': 20000\n",
    "}\n",
    "causal_margin_hyperparams_dict = {\n",
    "    'RQS_knots': 4,\n",
    "    'flow_layers': 8,\n",
    "    'nn_depth': 10,    \n",
    "    'nn_width': 50,\n",
    "}\n",
    "seed=7\n",
    "\n",
    "# Load data\n",
    "e401k = pd.read_csv('./data_processing_and_simulations/filtered_401k_data.csv')\n",
    "\n",
    "# Preprocess data\n",
    "outcome_col = 'net_tfa'\n",
    "treatment_col = 'e401'\n",
    "standardised_outcome_col = f'{outcome_col}_standardised'\n",
    "Y_control = e401k.loc[e401k[treatment_col]==0, outcome_col]\n",
    "Y_control_mean = Y_control.mean()\n",
    "Y_control_std = Y_control.std()\n",
    "e401k[standardised_outcome_col] = (e401k[outcome_col] - Y_control_mean) / Y_control_std\n",
    "e401k_filtered = e401k.loc[(e401k[standardised_outcome_col] > -2) & (e401k[standardised_outcome_col] < +3)]\n",
    "X = jnp.array(e401k_filtered[treatment_col].values)[:, None]\n",
    "Y = jnp.array(e401k_filtered[standardised_outcome_col].values)[:, None]\n",
    "Z_disc = jnp.array(e401k_filtered[[col for col in e401k_filtered.columns if col not in [outcome_col,standardised_outcome_col, treatment_col]]].values)\n",
    "\n",
    "\n",
    "true_ATE = 1000\n",
    "benchmark_flow = FrugalFlowModel(Y=Y, X=X, Z_disc=Z_disc, Z_cont=None, confounding_copula=None)\n",
    "benchmark_flow.train_benchmark_model(\n",
    "    training_seed=jr.PRNGKey(seed),\n",
    "    marginal_hyperparam_dict=hyperparam_dict,\n",
    "    frugal_hyperparam_dict=hyperparam_dict,\n",
    "    causal_model='location_translation',\n",
    "    causal_model_args={'ate': 0, **causal_margin_hyperparams_dict},\n",
    "    prop_flow_hyperparam_dict=hyperparam_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f27b4b-2005-4491-8d50-1529727b4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_outcome(x, mean, std):\n",
    "    return x * std + mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3fe7f3-e6b2-4942-8912-807a4ad07966",
   "metadata": {},
   "source": [
    "### Unconfounded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198d01ae-2246-4f76-9d0b-80bed287bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_confounded_results_list = []\n",
    "for seed in range(B):\n",
    "    print(f\"Run {seed+1} / {B}\")\n",
    "    sim_data_df = benchmark_flow.generate_samples(\n",
    "        key=jr.PRNGKey(10*seed),\n",
    "        sampling_size=sampling_size,\n",
    "        copula_param=0.,\n",
    "        outcome_causal_model='location_translation',\n",
    "        outcome_causal_args={'ate': true_ATE / Y_control_std},\n",
    "        with_confounding=False\n",
    "    )\n",
    "    sim_data_df['Y'] = sim_data_df['Y'].apply(lambda x: rescale_outcome(x, Y_control_mean, Y_control_std))\n",
    "    with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "        model_fits = valMethods.run_model_fits('Y', 'X', sim_data_df, sample_frac=1, repeats=1, replace=True)\n",
    "    ate_reslts = pd.concat([\n",
    "        model_fits['nonbootstrap_results'][['method', 'ate']], \n",
    "        model_fits['bootstrap_results'][['method', 'ate']]\n",
    "    ])\n",
    "    non_confounded_results_list.append(ate_reslts)\n",
    "non_confounded_model_fit_results = pd.concat(non_confounded_results_list)\n",
    "non_confounded_model_fit_results = non_confounded_model_fit_results.loc[\n",
    "    ~non_confounded_model_fit_results['method'].isin(['Gradient Boosting Trees DML', 'Doubly Robust (Linear)'])\n",
    "]\n",
    "non_confounded_model_fit_results['ate'] = non_confounded_model_fit_results['ate'].apply(clean_ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072ec25c-b320-44a7-9e4b-e637d2d6417f",
   "metadata": {},
   "source": [
    "### Confounded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda792d3-3acb-40d2-aa69-39172d97a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounded_results_list = []\n",
    "for seed in range(B):\n",
    "    print(f\"Run {seed+1} / {B}\")\n",
    "    sim_data_df = benchmark_flow.generate_samples(\n",
    "        key=jr.PRNGKey(10*seed),\n",
    "        sampling_size=sampling_size,\n",
    "        copula_param=0.,\n",
    "        outcome_causal_model='location_translation',\n",
    "        outcome_causal_args={'ate': true_ATE / Y_control_std},\n",
    "        with_confounding=True\n",
    "    )\n",
    "    sim_data_df['Y'] = sim_data_df['Y'].apply(lambda x: rescale_outcome(x, Y_control_mean, Y_control_std))    \n",
    "    with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "        model_fits = valMethods.run_model_fits('Y', 'X', sim_data_df, sample_frac=1, repeats=1, replace=True)\n",
    "    ate_reslts = pd.concat([\n",
    "        model_fits['nonbootstrap_results'][['method', 'ate']], \n",
    "        model_fits['bootstrap_results'][['method', 'ate']]\n",
    "    ])\n",
    "    confounded_results_list.append(ate_reslts)\n",
    "confounded_model_fit_results = pd.concat(confounded_results_list)\n",
    "confounded_model_fit_results = confounded_model_fit_results.loc[\n",
    "    ~confounded_model_fit_results['method'].isin(['Gradient Boosting Trees DML', 'Doubly Robust (Linear)'])\n",
    "]\n",
    "confounded_model_fit_results['ate'] = confounded_model_fit_results['ate'].apply(clean_ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a152104-1a86-40cc-a277-97c09b3e553f",
   "metadata": {},
   "source": [
    "### Hidden Confounded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19fb6e7-2229-46d4-b022-58acbceb7d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 0.2\n",
    "hidden_confounded_results_list = []\n",
    "for seed in range(B):\n",
    "    print(f\"Run {seed+1} / {B}\")\n",
    "    sim_data_df = benchmark_flow.generate_samples(\n",
    "        key=jr.PRNGKey(10*seed),\n",
    "        sampling_size=sampling_size,\n",
    "        copula_param=rho,\n",
    "        outcome_causal_model='location_translation',\n",
    "        outcome_causal_args={'ate': true_ATE / Y_control_std},\n",
    "        with_confounding=True\n",
    "    )\n",
    "    sim_data_df['Y'] = sim_data_df['Y'].apply(lambda x: rescale_outcome(x, Y_control_mean, Y_control_std))    \n",
    "    with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "        model_fits = valMethods.run_model_fits('Y', 'X', sim_data_df, sample_frac=1, repeats=1, replace=True)\n",
    "    ate_reslts = pd.concat([\n",
    "        model_fits['nonbootstrap_results'][['method', 'ate']], \n",
    "        model_fits['bootstrap_results'][['method', 'ate']]\n",
    "    ])\n",
    "    hidden_confounded_results_list.append(ate_reslts)\n",
    "hidden_confounded_model_fit_results = pd.concat(hidden_confounded_results_list)\n",
    "hidden_confounded_model_fit_results = hidden_confounded_model_fit_results.loc[\n",
    "    ~hidden_confounded_model_fit_results['method'].isin(['Gradient Boosting Trees DML', 'Doubly Robust (Linear)'])\n",
    "]\n",
    "hidden_confounded_model_fit_results['ate'] = hidden_confounded_model_fit_results['ate'].apply(clean_ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ea816a-a7cf-4596-82f0-8ad1dccfa232",
   "metadata": {},
   "source": [
    "## Some demo plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595b056-8e33-4c42-9126-9fb807b861b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['text.usetex'] = True\n",
    "\n",
    "def clean_ate(value):\n",
    "    if isinstance(value, (list, tuple, np.ndarray)):\n",
    "        return np.mean(value)\n",
    "    return value\n",
    "\n",
    "# Apply the cleaning function to the data\n",
    "df1 = non_confounded_model_fit_results.copy()\n",
    "df2 = confounded_model_fit_results.copy()\n",
    "df3 = hidden_confounded_model_fit_results.copy()\n",
    "\n",
    "# Group data by method\n",
    "grouped_df1 = df1.groupby('method')['ate'].apply(list).reset_index()\n",
    "grouped_df2 = df2.groupby('method')['ate'].apply(list).reset_index()\n",
    "grouped_df3 = df3.groupby('method')['ate'].apply(list).reset_index()\n",
    "\n",
    "# Plot the box and whisker diagrams side by side\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(14, 6), sharey=True)\n",
    "\n",
    "# Define a common font size\n",
    "font_size = 16\n",
    "title_fontsize = font_size + 2\n",
    "\n",
    "# Create the boxplot for the first dataset\n",
    "axes[0].boxplot(grouped_df1['ate'], vert=False, patch_artist=True, labels=grouped_df1['method'])\n",
    "axes[0].axvline(x=true_ATE, color='red', linestyle='--')\n",
    "axes[0].set_xlabel('ATE', fontsize=font_size)\n",
    "axes[0].set_title('No Confounding', fontsize=title_fontsize)\n",
    "axes[0].tick_params(axis='both', which='major', labelsize=font_size)\n",
    "axes[0].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Create the boxplot for the second dataset\n",
    "axes[1].boxplot(grouped_df2['ate'], vert=False, patch_artist=True, labels=grouped_df2['method'])\n",
    "axes[1].axvline(x=true_ATE, color='red', linestyle='--')\n",
    "axes[1].set_xlabel('ATE', fontsize=font_size)\n",
    "axes[1].set_title(r'With Real-World Confounding', fontsize=title_fontsize)\n",
    "axes[1].tick_params(axis='both', which='major', labelsize=font_size)\n",
    "axes[1].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Create the boxplot for the third dataset\n",
    "axes[2].boxplot(grouped_df3['ate'], vert=False, patch_artist=True, labels=grouped_df3['method'])\n",
    "axes[2].axvline(x=true_ATE, color='red', linestyle='--')\n",
    "axes[2].set_xlabel('ATE', fontsize=font_size)\n",
    "axes[2].set_title(r'With Hidden Confounding $\\rho=0.2$', fontsize=title_fontsize)\n",
    "axes[2].tick_params(axis='both', which='major', labelsize=font_size)\n",
    "axes[2].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF\n",
    "plt.savefig('e401k_box_and_whisker_plots.pdf', format='pdf')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537fb63d-37cb-44a1-8e4b-1b51d4534eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
