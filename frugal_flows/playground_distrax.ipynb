{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a6f7bfb-0d62-4494-8865-23fff84d5b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import distrax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1acdb572-3432-464f-acea-6210fd5b88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from flowjax.train import fit_to_data\n",
    "from flowjax.train.losses import MaximumLikelihoodLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abb3a175-eda2-4266-8921-0e9050e8058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "from typing import Callable, Optional, Sequence\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "from jax.random import PRNGKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdfcfed5-579e-4cb0-9843-bd6f15e0c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPConditioner(eqx.Module):\n",
    "    \"\"\"Multi-Layer Perceptron (MLP) Conditioner using Equinox.\n",
    "\n",
    "    This conditioner takes the flow input, passes it through an MLP, and produces\n",
    "    parameters for the bijector as required. Typically used with Masked Coupling Bijector\n",
    "    to ensure the lower triangular Jacobian is preserved.\n",
    "    \"\"\"\n",
    "\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim: int,\n",
    "        hidden_sizes: Sequence[int],\n",
    "        num_bijector_params: int,\n",
    "        key: PRNGKey,\n",
    "        activation: Callable = jax.nn.relu,\n",
    "        final_activation: Callable = lambda x: x,\n",
    "        name: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        depth = len(hidden_sizes) + 1  # Including the output layer\n",
    "        width_size = (\n",
    "            hidden_sizes[0] if hidden_sizes else output_dim\n",
    "        )  # Handle depth=1 case\n",
    "\n",
    "        # The last layer's size is determined by output_dim and num_bijector_params\n",
    "        out_size = output_dim * num_bijector_params\n",
    "        # MLP setup\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=\"scalar\",  # Assuming input to MLPConditioner is already flattened\n",
    "            out_size=out_size,  # Output size is tailored for the bijector parameterization\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=activation,\n",
    "            final_activation=final_activation,\n",
    "            use_bias=True,\n",
    "            use_final_bias=True,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, *, key: Optional[PRNGKey] = None):\n",
    "        # Assuming x has shape (batch_size, output_dim), and we're flattening it\n",
    "        # to work with Equinox's MLP expecting scalar inputs. Adjust as necessary.\n",
    "        x = x.reshape(\n",
    "            (x.shape[0], -1)\n",
    "        )  # Flatten if necessary, preserving batch dimension\n",
    "        out = self.mlp(x, key=key)  # Key is optional, here for compatibility\n",
    "        # Reshaping the output to have shape (batch_size, output_dim, num_bijector_params)\n",
    "        out = out.reshape((x.shape[0], -1, num_bijector_params))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa53afe3-988f-455a-91e5-8983bbc25887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPConditioner(\n",
       "  mlp=MLP(\n",
       "    layers=(\n",
       "      Linear(\n",
       "        weight=f32[5,1],\n",
       "        bias=f32[5],\n",
       "        in_features='scalar',\n",
       "        out_features=5,\n",
       "        use_bias=True\n",
       "      ),\n",
       "      Linear(\n",
       "        weight=f32[5,5],\n",
       "        bias=f32[5],\n",
       "        in_features=5,\n",
       "        out_features=5,\n",
       "        use_bias=True\n",
       "      ),\n",
       "      Linear(\n",
       "        weight=f32[5,5],\n",
       "        bias=f32[5],\n",
       "        in_features=5,\n",
       "        out_features=5,\n",
       "        use_bias=True\n",
       "      ),\n",
       "      Linear(\n",
       "        weight=f32[5,5],\n",
       "        bias=f32[5],\n",
       "        in_features=5,\n",
       "        out_features=5,\n",
       "        use_bias=True\n",
       "      ),\n",
       "      Linear(\n",
       "        weight=f32[62,5],\n",
       "        bias=f32[62],\n",
       "        in_features=5,\n",
       "        out_features=62,\n",
       "        use_bias=True\n",
       "      )\n",
       "    ),\n",
       "    activation=<wrapped function relu>,\n",
       "    final_activation=<function <lambda>>,\n",
       "    use_bias=True,\n",
       "    use_final_bias=True,\n",
       "    in_size='scalar',\n",
       "    out_size=62,\n",
       "    width_size=5,\n",
       "    depth=4\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLPConditioner(\n",
    "    key=PRNGKey(9),\n",
    "    output_dim=math.prod((2,)),\n",
    "    hidden_sizes=[5, 5, 5],\n",
    "    num_bijector_params=3 * 10 + 1,\n",
    "    name=\"conditioner_phi\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb61bbb2-6150-477a-8427-5140d3996ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NSF(\n",
    "    key: PRNGKey,\n",
    "    phi_dim: int,\n",
    "    num_layers: int,\n",
    "    hidden_sizes: Sequence[int],\n",
    "    num_bins: int,\n",
    "    range_min: float = 0.0,\n",
    "    range_max: float = 1.0,\n",
    "    **_,\n",
    ") -> distrax.Transformed:\n",
    "    \"\"\"Creates the Rational Quadratic Flow model.\n",
    "\n",
    "    Args:\n",
    "    range_min: the lower bound of the spline's range. Below `range_min`, the\n",
    "      bijector defaults to a linear transformation.\n",
    "    range_max: the upper bound of the spline's range. Above `range_max`, the\n",
    "      bijector defaults to a linear transformation.\n",
    "    \"\"\"\n",
    "\n",
    "    flow_dim = phi_dim\n",
    "\n",
    "    event_shape = (flow_dim,)\n",
    "\n",
    "    flow_layers = []\n",
    "\n",
    "    # Number of parameters required by the bijector (rational quadratic spline)\n",
    "    num_bijector_params = 3 * num_bins + 1\n",
    "\n",
    "    def bijector_fn(params):\n",
    "        return distrax.RationalQuadraticSpline(\n",
    "            params, range_min=range_min, range_max=range_max\n",
    "        )\n",
    "\n",
    "    # Alternating binary mask.\n",
    "    mask = jnp.arange(0, math.prod(event_shape)) % 2\n",
    "    mask = jnp.reshape(mask, event_shape)\n",
    "    mask = mask.astype(bool)\n",
    "\n",
    "    # Number of parameters for the rational-quadratic spline:\n",
    "    # - `num_bins` bin widths\n",
    "    # - `num_bins` bin heights\n",
    "    # - `num_bins + 1` knot slopes\n",
    "    # for a total of `3 * num_bins + 1` parameters.\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        layer = distrax.MaskedCoupling(\n",
    "            mask=mask,\n",
    "            bijector=bijector_fn,\n",
    "            conditioner=MLPConditioner(\n",
    "                key=key,\n",
    "                output_dim=math.prod(event_shape),\n",
    "                hidden_sizes=hidden_sizes,\n",
    "                num_bijector_params=num_bijector_params,\n",
    "                name=\"conditioner_phi\",\n",
    "            ),\n",
    "        )\n",
    "        flow_layers.append(layer)\n",
    "        # Flip the mask after each layer.\n",
    "        mask = jnp.logical_not(mask)\n",
    "\n",
    "    # Last layer: Map values to parameter domain\n",
    "    # phi goes to [0,1]\n",
    "    flow_layers.append(distrax.Block(distrax.Sigmoid(), 1))\n",
    "\n",
    "    flow = distrax.Chain(flow_layers[::-1])\n",
    "\n",
    "    # base_distribution = distrax.Independent(\n",
    "    #     distrax.Uniform(low=jnp.zeros(event_shape), high=jnp.ones(event_shape)),\n",
    "    #     reinterpreted_batch_ndims=len(event_shape))\n",
    "\n",
    "    base_distribution = distrax.MultivariateNormalDiag(\n",
    "        loc=jnp.zeros(event_shape), scale_diag=jnp.ones(event_shape)\n",
    "    )\n",
    "\n",
    "    return distrax.Transformed(base_distribution, flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c5bb602-06ad-45c0-a150-6cbeb544f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvars = 2\n",
    "key, x_key = jr.split(jr.PRNGKey(0))\n",
    "x = jr.normal(x_key, shape=(5000, nvars))\n",
    "# x = jr.beta(x_key, a=0.4, b=0.4, shape=(5000, nvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee482a97-27c2-45be-849a-a3554672984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jr.split(jr.PRNGKey(0))\n",
    "\n",
    "# Create the flow\n",
    "untrained_flow = NSF(\n",
    "    key=subkey,\n",
    "    phi_dim=2,  # x dim\n",
    "    num_layers=8,\n",
    "    hidden_sizes=[5, 5, 5],\n",
    "    num_bins=10,\n",
    "    range_min=0.0,\n",
    "    range_max=1.0,\n",
    ")\n",
    "\n",
    "nsf_constructor = partial(\n",
    "    NSF,\n",
    "    phi_dim=2,  # Example configuration; adjust as needed\n",
    "    num_layers=3,\n",
    "    hidden_sizes=[64, 64],\n",
    "    num_bins=10,\n",
    "    range_min=0.0,\n",
    "    range_max=1.0,\n",
    "    # Include other parameters as required by your NSF model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f01009a-bc11-4e9f-a8b1-06375b848c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<distrax._src.distributions.transformed.Transformed at 0x1899fff50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untrained_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd4886f9-ee2b-4b47-b305-934a913641bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def fit_to_data(\n",
    "    key: jnp.ndarray,\n",
    "    nsf_constructor: Callable,\n",
    "    x: jnp.ndarray,\n",
    "    *,\n",
    "    condition: Optional[jnp.ndarray] = None,\n",
    "    max_epochs: int = 100,\n",
    "    batch_size: int = 100,\n",
    "    learning_rate: float = 5e-4,\n",
    "    optimizer: Optional[optax.GradientTransformation] = None,\n",
    "    return_best: bool = True,\n",
    "    show_progress: bool = True,\n",
    "):\n",
    "    # Assuming the NSF constructor accepts a PRNGKey and returns an initialized flow\n",
    "    nsf_flow = nsf_constructor(key=key)\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = optax.adam(learning_rate)\n",
    "\n",
    "    loss_fn = MaximumLikelihoodLoss()\n",
    "\n",
    "    params, static = eqx.partition(nsf_flow, eqx.is_array)\n",
    "    opt_state = optimizer.init(params)\n",
    "    best_params, best_loss = params, jnp.inf\n",
    "\n",
    "    for epoch in tqdm(range(max_epochs), disable=not show_progress):\n",
    "        # Shuffle the data at the beginning of each epoch\n",
    "        indices = jr.permutation(key, len(x))\n",
    "        x_shuffled = x[indices]\n",
    "\n",
    "        epoch_loss = []\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            batch_x = x_shuffled[i : i + batch_size]\n",
    "            batch_cond = (\n",
    "                condition[i : i + batch_size] if condition is not None else None\n",
    "            )\n",
    "\n",
    "            # Gradient update step\n",
    "            grads, batch_loss = jax.value_and_grad(loss_fn, argnums=0)(\n",
    "                params, static, batch_x, batch_cond\n",
    "            )\n",
    "            updates, opt_state = optimizer.update(grads, opt_state)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "\n",
    "            epoch_loss.append(batch_loss)\n",
    "\n",
    "        epoch_loss = jnp.mean(jnp.stack(epoch_loss))\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_params = params\n",
    "\n",
    "        tqdm.write(f\"Epoch {epoch+1}, Loss: {epoch_loss}\")\n",
    "\n",
    "    if return_best:\n",
    "        trained_flow = eqx.combine(static, best_params)\n",
    "    else:\n",
    "        trained_flow = eqx.combine(static, params)\n",
    "\n",
    "    return trained_flow, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0c2f653-858e-46b5-a68e-40b15c55ce36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<distrax._src.distributions.transformed.Transformed at 0x18a567f50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untrained_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "caef78d8-2f28-4a78-ab13-10f435b1b682",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatch custom node data: ([], [], PyTreeDef({'_batch_shape': None, '_bijector': CustomNode(Chain[([1, 1, False, False], [False, False, False, False], PyTreeDef({'_bijectors': [CustomNode(Block[([1, 1, False, False, 1], [False, False, False, False, False], PyTreeDef({'_bijector': CustomNode(Sigmoid[([0, 0, False, False], [False, False, False, False], PyTreeDef({'_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *}))], [None, None, None, None]), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_ndims': *}))], [None, None, None, None, None]), CustomNode(MaskedCoupling[([<function NSF.<locals>.bijector_fn at 0x18a30aca0>, True, True, True, True, True, True, True, True, <jax._src.custom_derivatives.custom_jvp object at 0x14fcaea50>, <function MLPConditioner.<lambda> at 0x18a57e700>, True, 1, 1, 1, 0, False, False, True], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], PyTreeDef({'_bijector': *, '_conditioner': CustomNode(MLPConditioner[('mlp',), (), ()], [CustomNode(MLP[('layers', 'activation', 'final_activation'), ('use_bias', 'use_final_bias', 'in_size', 'out_size', 'width_size', 'depth'), (True, True, 'scalar', 62, 64, 3)], [(CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), ('scalar', 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 62, True)], [*, *])), *, *])]), '_event_mask': *, '_event_ndims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_inner_event_ndims': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_mask': *}))], [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]), CustomNode(MaskedCoupling[([<function NSF.<locals>.bijector_fn at 0x18a30aca0>, True, True, True, True, True, True, True, True, <jax._src.custom_derivatives.custom_jvp object at 0x14fcaea50>, <function MLPConditioner.<lambda> at 0x18a57e700>, True, 1, 1, 1, 0, False, False, True], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], PyTreeDef({'_bijector': *, '_conditioner': CustomNode(MLPConditioner[('mlp',), (), ()], [CustomNode(MLP[('layers', 'activation', 'final_activation'), ('use_bias', 'use_final_bias', 'in_size', 'out_size', 'width_size', 'depth'), (True, True, 'scalar', 62, 64, 3)], [(CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), ('scalar', 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 62, True)], [*, *])), *, *])]), '_event_mask': *, '_event_ndims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_inner_event_ndims': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_mask': *}))], [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]), CustomNode(MaskedCoupling[([<function NSF.<locals>.bijector_fn at 0x18a30aca0>, True, True, True, True, True, True, True, True, <jax._src.custom_derivatives.custom_jvp object at 0x14fcaea50>, <function MLPConditioner.<lambda> at 0x18a57e700>, True, 1, 1, 1, 0, False, False, True], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], PyTreeDef({'_bijector': *, '_conditioner': CustomNode(MLPConditioner[('mlp',), (), ()], [CustomNode(MLP[('layers', 'activation', 'final_activation'), ('use_bias', 'use_final_bias', 'in_size', 'out_size', 'width_size', 'depth'), (True, True, 'scalar', 62, 64, 3)], [(CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), ('scalar', 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 62, True)], [*, *])), *, *])]), '_event_mask': *, '_event_ndims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_inner_event_ndims': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_mask': *}))], [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None])], '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *}))], [None, None, None, None]), '_distribution': CustomNode(MultivariateNormalDiag[([dtype('float32'), 2, True, True], [False, False, False, False], PyTreeDef({'_batch_shape': (), '_bijector': CustomNode(Chain[([1, 1, True, True], [False, False, False, False], PyTreeDef({'_bijectors': [CustomNode(Block[([1, 1, True, True, 1], [False, False, False, False, False], PyTreeDef({'_bijector': CustomNode(Shift[([2, 0, 0, True, True, True], [False, False, False, False, False, False], PyTreeDef({'_batch_shape': (*,), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_shift': *}))], [None, None, None, None, None, None]), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_ndims': *}))], [None, None, None, None, None]), CustomNode(DiagLinear[([True, dtype('float32'), 2, 1, 1, True, True, <bound method Block.forward of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.forward_log_det_jacobian of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse_and_log_det of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse_log_det_jacobian of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>], [False, False, False, False, False, False, False, False, False, False, False, False], PyTreeDef({'_batch_shape': (), '_bijector': CustomNode(Block[([1, 1, True, True, 1], [False, False, False, False, False], PyTreeDef({'_bijector': CustomNode(ScalarAffine[([2, 0, 0, True, True, True, True, True, 0.0], [False, False, False, False, False, False, False, False, False], PyTreeDef({'_batch_shape': (*,), '_event_ndims_in': *, '_event_ndims_out': *, '_inv_scale': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_log_scale': *, '_scale': *, '_shift': *}))], [None, None, None, None, None, None, None, None, None]), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_ndims': *}))], [None, None, None, None, None]), '_diag': *, '_dtype': *, '_event_dims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, 'forward': *, 'forward_log_det_jacobian': *, 'inverse': *, 'inverse_and_log_det': *, 'inverse_log_det_jacobian': *}))], [None, None, None, None, None, None, None, None, None, None, None, None])], '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *}))], [None, None, None, None]), '_distribution': CustomNode(Independent[([1], [False], PyTreeDef({'_distribution': CustomNode(Normal[([True, True], [False, False], PyTreeDef({'_loc': *, '_scale': *}))], [None, None]), '_reinterpreted_batch_ndims': *}))], [None]), '_dtype': *, '_event_shape': (*,), '_loc': *, '_scale': CustomNode(DiagLinear[([True, dtype('float32'), 2, 1, 1, True, True, <bound method Block.forward of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.forward_log_det_jacobian of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse_and_log_det of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse_log_det_jacobian of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>], [False, False, False, False, False, False, False, False, False, False, False, False], PyTreeDef({'_batch_shape': (), '_bijector': CustomNode(Block[([1, 1, True, True, 1], [False, False, False, False, False], PyTreeDef({'_bijector': CustomNode(ScalarAffine[([2, 0, 0, True, True, True, True, True, 0.0], [False, False, False, False, False, False, False, False, False], PyTreeDef({'_batch_shape': (*,), '_event_ndims_in': *, '_event_ndims_out': *, '_inv_scale': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_log_scale': *, '_scale': *, '_shift': *}))], [None, None, None, None, None, None, None, None, None]), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_ndims': *}))], [None, None, None, None, None]), '_diag': *, '_dtype': *, '_event_dims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, 'forward': *, 'forward_log_det_jacobian': *, 'inverse': *, 'inverse_and_log_det': *, 'inverse_log_det_jacobian': *}))], [None, None, None, None, None, None, None, None, None, None, None, None]), '_scale_diag': *}))], [None, None, None, None]), '_dtype': None, '_event_shape': None})) != ([None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True], PyTreeDef({'_batch_shape': None, '_bijector': CustomNode(Chain[([None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 1, 1, False, False], [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False], PyTreeDef({'_bijectors': [CustomNode(Block[([1, 1, False, False, 1], [False, False, False, False, False], PyTreeDef({'_bijector': CustomNode(Sigmoid[([0, 0, False, False], [False, False, False, False], PyTreeDef({'_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *}))], [None, None, None, None]), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_ndims': *}))], [None, None, None, None, None]), CustomNode(MaskedCoupling[([<function NSF.<locals>.bijector_fn at 0x18a30aca0>, None, None, None, None, None, None, None, None, <jax._src.custom_derivatives.custom_jvp object at 0x14fcaea50>, <function MLPConditioner.<lambda> at 0x18a57e700>, None, 1, 1, 1, 0, False, False, None], [False, True, True, True, True, True, True, True, True, False, False, True, False, False, False, False, False, False, True], PyTreeDef({'_bijector': *, '_conditioner': CustomNode(MLPConditioner[('mlp',), (), ()], [CustomNode(MLP[('layers', 'activation', 'final_activation'), ('use_bias', 'use_final_bias', 'in_size', 'out_size', 'width_size', 'depth'), (True, True, 'scalar', 62, 64, 3)], [(CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), ('scalar', 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 62, True)], [*, *])), *, *])]), '_event_mask': *, '_event_ndims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_inner_event_ndims': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_mask': *}))], [None, *, *, *, *, *, *, *, *, None, None, *, None, None, None, None, None, None, *]), CustomNode(MaskedCoupling[([<function NSF.<locals>.bijector_fn at 0x18a30aca0>, None, None, None, None, None, None, None, None, <jax._src.custom_derivatives.custom_jvp object at 0x14fcaea50>, <function MLPConditioner.<lambda> at 0x18a57e700>, None, 1, 1, 1, 0, False, False, None], [False, True, True, True, True, True, True, True, True, False, False, True, False, False, False, False, False, False, True], PyTreeDef({'_bijector': *, '_conditioner': CustomNode(MLPConditioner[('mlp',), (), ()], [CustomNode(MLP[('layers', 'activation', 'final_activation'), ('use_bias', 'use_final_bias', 'in_size', 'out_size', 'width_size', 'depth'), (True, True, 'scalar', 62, 64, 3)], [(CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), ('scalar', 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 62, True)], [*, *])), *, *])]), '_event_mask': *, '_event_ndims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_inner_event_ndims': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_mask': *}))], [None, *, *, *, *, *, *, *, *, None, None, *, None, None, None, None, None, None, *]), CustomNode(MaskedCoupling[([<function NSF.<locals>.bijector_fn at 0x18a30aca0>, None, None, None, None, None, None, None, None, <jax._src.custom_derivatives.custom_jvp object at 0x14fcaea50>, <function MLPConditioner.<lambda> at 0x18a57e700>, None, 1, 1, 1, 0, False, False, None], [False, True, True, True, True, True, True, True, True, False, False, True, False, False, False, False, False, False, True], PyTreeDef({'_bijector': *, '_conditioner': CustomNode(MLPConditioner[('mlp',), (), ()], [CustomNode(MLP[('layers', 'activation', 'final_activation'), ('use_bias', 'use_final_bias', 'in_size', 'out_size', 'width_size', 'depth'), (True, True, 'scalar', 62, 64, 3)], [(CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), ('scalar', 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 62, True)], [*, *])), *, *])]), '_event_mask': *, '_event_ndims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_inner_event_ndims': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_mask': *}))], [None, *, *, *, *, *, *, *, *, None, None, *, None, None, None, None, None, None, *])], '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *}))], [*, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, None, None, None, None]), '_distribution': CustomNode(MultivariateNormalDiag[([None, None, None, None, None, None, None, dtype('float32'), 2, None, None, None, None, None, None], [True, True, True, True, True, True, True, False, False, True, True, True, True, True, True], PyTreeDef({'_batch_shape': (), '_bijector': CustomNode(Chain[([None, None, None, None, None, 1, 1, True, True], [True, True, True, True, True, False, False, False, False], PyTreeDef({'_bijectors': [CustomNode(Block[([None, 1, 1, True, True, 1], [True, False, False, False, False, False], PyTreeDef({'_bijector': CustomNode(Shift[([2, 0, 0, True, True, None], [False, False, False, False, False, True], PyTreeDef({'_batch_shape': (*,), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_shift': *}))], [None, None, None, None, None, *]), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_ndims': *}))], [*, None, None, None, None, None]), CustomNode(DiagLinear[([None, None, None, None, dtype('float32'), 2, 1, 1, True, True, <bound method Block.forward of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.forward_log_det_jacobian of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse_and_log_det of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse_log_det_jacobian of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>], [True, True, True, True, False, False, False, False, False, False, False, False, False, False, False], PyTreeDef({'_batch_shape': (), '_bijector': CustomNode(Block[([None, None, None, 1, 1, True, True, 1], [True, True, True, False, False, False, False, False], PyTreeDef({'_bijector': CustomNode(ScalarAffine[([2, 0, 0, None, True, True, None, None, 0.0], [False, False, False, True, False, False, True, True, False], PyTreeDef({'_batch_shape': (*,), '_event_ndims_in': *, '_event_ndims_out': *, '_inv_scale': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_log_scale': *, '_scale': *, '_shift': *}))], [None, None, None, *, None, None, *, *, None]), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_ndims': *}))], [*, *, *, None, None, None, None, None]), '_diag': *, '_dtype': *, '_event_dims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, 'forward': *, 'forward_log_det_jacobian': *, 'inverse': *, 'inverse_and_log_det': *, 'inverse_log_det_jacobian': *}))], [*, *, *, *, None, None, None, None, None, None, None, None, None, None, None])], '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *}))], [*, *, *, *, *, None, None, None, None]), '_distribution': CustomNode(Independent[([None, None, 1], [True, True, False], PyTreeDef({'_distribution': CustomNode(Normal[([None, None], [True, True], PyTreeDef({'_loc': *, '_scale': *}))], [*, *]), '_reinterpreted_batch_ndims': *}))], [*, *, None]), '_dtype': *, '_event_shape': (*,), '_loc': *, '_scale': CustomNode(DiagLinear[([None, None, None, None, dtype('float32'), 2, 1, 1, True, True, <bound method Block.forward of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.forward_log_det_jacobian of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse_and_log_det of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse_log_det_jacobian of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>], [True, True, True, True, False, False, False, False, False, False, False, False, False, False, False], PyTreeDef({'_batch_shape': (), '_bijector': CustomNode(Block[([None, None, None, 1, 1, True, True, 1], [True, True, True, False, False, False, False, False], PyTreeDef({'_bijector': CustomNode(ScalarAffine[([2, 0, 0, None, True, True, None, None, 0.0], [False, False, False, True, False, False, True, True, False], PyTreeDef({'_batch_shape': (*,), '_event_ndims_in': *, '_event_ndims_out': *, '_inv_scale': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_log_scale': *, '_scale': *, '_shift': *}))], [None, None, None, *, None, None, *, *, None]), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_ndims': *}))], [*, *, *, None, None, None, None, None]), '_diag': *, '_dtype': *, '_event_dims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, 'forward': *, 'forward_log_det_jacobian': *, 'inverse': *, 'inverse_and_log_det': *, 'inverse_log_det_jacobian': *}))], [*, *, *, *, None, None, None, None, None, None, None, None, None, None, None]), '_scale_diag': *}))], [*, *, *, *, *, *, *, None, None, *, *, *, *, *, *]), '_dtype': None, '_event_shape': None})); value: <distrax._src.distributions.transformed.Transformed object at 0x18a8839d0>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m key, subkey \u001b[38;5;241m=\u001b[39m jr\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Train on the unbounded space\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m flow, losses \u001b[38;5;241m=\u001b[39m \u001b[43mfit_to_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnsf_constructor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnsf_constructor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_best\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 25\u001b[0m, in \u001b[0;36mfit_to_data\u001b[0;34m(key, nsf_constructor, x, condition, max_epochs, batch_size, learning_rate, optimizer, return_best, show_progress)\u001b[0m\n\u001b[1;32m     21\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39madam(learning_rate)\n\u001b[1;32m     23\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m MaximumLikelihoodLoss()\n\u001b[0;32m---> 25\u001b[0m params, static \u001b[38;5;241m=\u001b[39m \u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnsf_flow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39minit(params)\n\u001b[1;32m     27\u001b[0m best_params, best_loss \u001b[38;5;241m=\u001b[39m params, jnp\u001b[38;5;241m.\u001b[39minf\n",
      "File \u001b[0;32m~/micromamba/envs/deep-frugal/lib/python3.11/site-packages/equinox/_filters.py:150\u001b[0m, in \u001b[0;36mpartition\u001b[0;34m(pytree, filter_spec, replace, is_leaf)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Splits a PyTree into two pieces. Equivalent to\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m`filter(...), filter(..., inverse=True)`, but slightly more efficient.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    See also [`equinox.combine`][] to reconstitute the PyTree again.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m filter_tree \u001b[38;5;241m=\u001b[39m jtu\u001b[38;5;241m.\u001b[39mtree_map(_make_filter_tree(is_leaf), filter_spec, pytree)\n\u001b[0;32m--> 150\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[43mjtu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpytree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m right \u001b[38;5;241m=\u001b[39m jtu\u001b[38;5;241m.\u001b[39mtree_map(\u001b[38;5;28;01mlambda\u001b[39;00m mask, x: replace \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;28;01melse\u001b[39;00m x, filter_tree, pytree)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m left, right\n",
      "File \u001b[0;32m~/micromamba/envs/deep-frugal/lib/python3.11/site-packages/jax/_src/tree_util.py:311\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maps a multi-input function over pytree args to produce a new pytree.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m  - :func:`jax.tree.reduce`\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    310\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[0;32m--> 311\u001b[0m all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m \u001b[43m[\u001b[49m\u001b[43mtreedef\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten_up_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m treedef\u001b[38;5;241m.\u001b[39munflatten(f(\u001b[38;5;241m*\u001b[39mxs) \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mall_leaves))\n",
      "File \u001b[0;32m~/micromamba/envs/deep-frugal/lib/python3.11/site-packages/jax/_src/tree_util.py:311\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maps a multi-input function over pytree args to produce a new pytree.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m  - :func:`jax.tree.reduce`\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    310\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[0;32m--> 311\u001b[0m all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [\u001b[43mtreedef\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten_up_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m treedef\u001b[38;5;241m.\u001b[39munflatten(f(\u001b[38;5;241m*\u001b[39mxs) \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mall_leaves))\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatch custom node data: ([], [], PyTreeDef({'_batch_shape': None, '_bijector': CustomNode(Chain[([1, 1, False, False], [False, False, False, False], PyTreeDef({'_bijectors': [CustomNode(Block[([1, 1, False, False, 1], [False, False, False, False, False], PyTreeDef({'_bijector': CustomNode(Sigmoid[([0, 0, False, False], [False, False, False, False], PyTreeDef({'_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *}))], [None, None, None, None]), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_ndims': *}))], [None, None, None, None, None]), CustomNode(MaskedCoupling[([<function NSF.<locals>.bijector_fn at 0x18a30aca0>, True, True, True, True, True, True, True, True, <jax._src.custom_derivatives.custom_jvp object at 0x14fcaea50>, <function MLPConditioner.<lambda> at 0x18a57e700>, True, 1, 1, 1, 0, False, False, True], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], PyTreeDef({'_bijector': *, '_conditioner': CustomNode(MLPConditioner[('mlp',), (), ()], [CustomNode(MLP[('layers', 'activation', 'final_activation'), ('use_bias', 'use_final_bias', 'in_size', 'out_size', 'width_size', 'depth'), (True, True, 'scalar', 62, 64, 3)], [(CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), ('scalar', 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 62, True)], [*, *])), *, *])]), '_event_mask': *, '_event_ndims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_inner_event_ndims': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_mask': *}))], [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]), CustomNode(MaskedCoupling[([<function NSF.<locals>.bijector_fn at 0x18a30aca0>, True, True, True, True, True, True, True, True, <jax._src.custom_derivatives.custom_jvp object at 0x14fcaea50>, <function MLPConditioner.<lambda> at 0x18a57e700>, True, 1, 1, 1, 0, False, False, True], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], PyTreeDef({'_bijector': *, '_conditioner': CustomNode(MLPConditioner[('mlp',), (), ()], [CustomNode(MLP[('layers', 'activation', 'final_activation'), ('use_bias', 'use_final_bias', 'in_size', 'out_size', 'width_size', 'depth'), (True, True, 'scalar', 62, 64, 3)], [(CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), ('scalar', 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 62, True)], [*, *])), *, *])]), '_event_mask': *, '_event_ndims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_inner_event_ndims': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_mask': *}))], [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]), CustomNode(MaskedCoupling[([<function NSF.<locals>.bijector_fn at 0x18a30aca0>, True, True, True, True, True, True, True, True, <jax._src.custom_derivatives.custom_jvp object at 0x14fcaea50>, <function MLPConditioner.<lambda> at 0x18a57e700>, True, 1, 1, 1, 0, False, False, True], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], PyTreeDef({'_bijector': *, '_conditioner': CustomNode(MLPConditioner[('mlp',), (), ()], [CustomNode(MLP[('layers', 'activation', 'final_activation'), ('use_bias', 'use_final_bias', 'in_size', 'out_size', 'width_size', 'depth'), (True, True, 'scalar', 62, 64, 3)], [(CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), ('scalar', 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 62, True)], [*, *])), *, *])]), '_event_mask': *, '_event_ndims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_inner_event_ndims': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_mask': *}))], [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None])], '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *}))], [None, None, None, None]), '_distribution': CustomNode(MultivariateNormalDiag[([dtype('float32'), 2, True, True], [False, False, False, False], PyTreeDef({'_batch_shape': (), '_bijector': CustomNode(Chain[([1, 1, True, True], [False, False, False, False], PyTreeDef({'_bijectors': [CustomNode(Block[([1, 1, True, True, 1], [False, False, False, False, False], PyTreeDef({'_bijector': CustomNode(Shift[([2, 0, 0, True, True, True], [False, False, False, False, False, False], PyTreeDef({'_batch_shape': (*,), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_shift': *}))], [None, None, None, None, None, None]), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_ndims': *}))], [None, None, None, None, None]), CustomNode(DiagLinear[([True, dtype('float32'), 2, 1, 1, True, True, <bound method Block.forward of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.forward_log_det_jacobian of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse_and_log_det of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse_log_det_jacobian of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>], [False, False, False, False, False, False, False, False, False, False, False, False], PyTreeDef({'_batch_shape': (), '_bijector': CustomNode(Block[([1, 1, True, True, 1], [False, False, False, False, False], PyTreeDef({'_bijector': CustomNode(ScalarAffine[([2, 0, 0, True, True, True, True, True, 0.0], [False, False, False, False, False, False, False, False, False], PyTreeDef({'_batch_shape': (*,), '_event_ndims_in': *, '_event_ndims_out': *, '_inv_scale': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_log_scale': *, '_scale': *, '_shift': *}))], [None, None, None, None, None, None, None, None, None]), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_ndims': *}))], [None, None, None, None, None]), '_diag': *, '_dtype': *, '_event_dims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, 'forward': *, 'forward_log_det_jacobian': *, 'inverse': *, 'inverse_and_log_det': *, 'inverse_log_det_jacobian': *}))], [None, None, None, None, None, None, None, None, None, None, None, None])], '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *}))], [None, None, None, None]), '_distribution': CustomNode(Independent[([1], [False], PyTreeDef({'_distribution': CustomNode(Normal[([True, True], [False, False], PyTreeDef({'_loc': *, '_scale': *}))], [None, None]), '_reinterpreted_batch_ndims': *}))], [None]), '_dtype': *, '_event_shape': (*,), '_loc': *, '_scale': CustomNode(DiagLinear[([True, dtype('float32'), 2, 1, 1, True, True, <bound method Block.forward of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.forward_log_det_jacobian of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse_and_log_det of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse_log_det_jacobian of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>], [False, False, False, False, False, False, False, False, False, False, False, False], PyTreeDef({'_batch_shape': (), '_bijector': CustomNode(Block[([1, 1, True, True, 1], [False, False, False, False, False], PyTreeDef({'_bijector': CustomNode(ScalarAffine[([2, 0, 0, True, True, True, True, True, 0.0], [False, False, False, False, False, False, False, False, False], PyTreeDef({'_batch_shape': (*,), '_event_ndims_in': *, '_event_ndims_out': *, '_inv_scale': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_log_scale': *, '_scale': *, '_shift': *}))], [None, None, None, None, None, None, None, None, None]), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_ndims': *}))], [None, None, None, None, None]), '_diag': *, '_dtype': *, '_event_dims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, 'forward': *, 'forward_log_det_jacobian': *, 'inverse': *, 'inverse_and_log_det': *, 'inverse_log_det_jacobian': *}))], [None, None, None, None, None, None, None, None, None, None, None, None]), '_scale_diag': *}))], [None, None, None, None]), '_dtype': None, '_event_shape': None})) != ([None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True], PyTreeDef({'_batch_shape': None, '_bijector': CustomNode(Chain[([None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 1, 1, False, False], [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False], PyTreeDef({'_bijectors': [CustomNode(Block[([1, 1, False, False, 1], [False, False, False, False, False], PyTreeDef({'_bijector': CustomNode(Sigmoid[([0, 0, False, False], [False, False, False, False], PyTreeDef({'_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *}))], [None, None, None, None]), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_ndims': *}))], [None, None, None, None, None]), CustomNode(MaskedCoupling[([<function NSF.<locals>.bijector_fn at 0x18a30aca0>, None, None, None, None, None, None, None, None, <jax._src.custom_derivatives.custom_jvp object at 0x14fcaea50>, <function MLPConditioner.<lambda> at 0x18a57e700>, None, 1, 1, 1, 0, False, False, None], [False, True, True, True, True, True, True, True, True, False, False, True, False, False, False, False, False, False, True], PyTreeDef({'_bijector': *, '_conditioner': CustomNode(MLPConditioner[('mlp',), (), ()], [CustomNode(MLP[('layers', 'activation', 'final_activation'), ('use_bias', 'use_final_bias', 'in_size', 'out_size', 'width_size', 'depth'), (True, True, 'scalar', 62, 64, 3)], [(CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), ('scalar', 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 62, True)], [*, *])), *, *])]), '_event_mask': *, '_event_ndims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_inner_event_ndims': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_mask': *}))], [None, *, *, *, *, *, *, *, *, None, None, *, None, None, None, None, None, None, *]), CustomNode(MaskedCoupling[([<function NSF.<locals>.bijector_fn at 0x18a30aca0>, None, None, None, None, None, None, None, None, <jax._src.custom_derivatives.custom_jvp object at 0x14fcaea50>, <function MLPConditioner.<lambda> at 0x18a57e700>, None, 1, 1, 1, 0, False, False, None], [False, True, True, True, True, True, True, True, True, False, False, True, False, False, False, False, False, False, True], PyTreeDef({'_bijector': *, '_conditioner': CustomNode(MLPConditioner[('mlp',), (), ()], [CustomNode(MLP[('layers', 'activation', 'final_activation'), ('use_bias', 'use_final_bias', 'in_size', 'out_size', 'width_size', 'depth'), (True, True, 'scalar', 62, 64, 3)], [(CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), ('scalar', 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 62, True)], [*, *])), *, *])]), '_event_mask': *, '_event_ndims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_inner_event_ndims': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_mask': *}))], [None, *, *, *, *, *, *, *, *, None, None, *, None, None, None, None, None, None, *]), CustomNode(MaskedCoupling[([<function NSF.<locals>.bijector_fn at 0x18a30aca0>, None, None, None, None, None, None, None, None, <jax._src.custom_derivatives.custom_jvp object at 0x14fcaea50>, <function MLPConditioner.<lambda> at 0x18a57e700>, None, 1, 1, 1, 0, False, False, None], [False, True, True, True, True, True, True, True, True, False, False, True, False, False, False, False, False, False, True], PyTreeDef({'_bijector': *, '_conditioner': CustomNode(MLPConditioner[('mlp',), (), ()], [CustomNode(MLP[('layers', 'activation', 'final_activation'), ('use_bias', 'use_final_bias', 'in_size', 'out_size', 'width_size', 'depth'), (True, True, 'scalar', 62, 64, 3)], [(CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), ('scalar', 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 64, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (64, 62, True)], [*, *])), *, *])]), '_event_mask': *, '_event_ndims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_inner_event_ndims': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_mask': *}))], [None, *, *, *, *, *, *, *, *, None, None, *, None, None, None, None, None, None, *])], '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *}))], [*, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, *, None, None, None, None]), '_distribution': CustomNode(MultivariateNormalDiag[([None, None, None, None, None, None, None, dtype('float32'), 2, None, None, None, None, None, None], [True, True, True, True, True, True, True, False, False, True, True, True, True, True, True], PyTreeDef({'_batch_shape': (), '_bijector': CustomNode(Chain[([None, None, None, None, None, 1, 1, True, True], [True, True, True, True, True, False, False, False, False], PyTreeDef({'_bijectors': [CustomNode(Block[([None, 1, 1, True, True, 1], [True, False, False, False, False, False], PyTreeDef({'_bijector': CustomNode(Shift[([2, 0, 0, True, True, None], [False, False, False, False, False, True], PyTreeDef({'_batch_shape': (*,), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_shift': *}))], [None, None, None, None, None, *]), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_ndims': *}))], [*, None, None, None, None, None]), CustomNode(DiagLinear[([None, None, None, None, dtype('float32'), 2, 1, 1, True, True, <bound method Block.forward of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.forward_log_det_jacobian of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse_and_log_det of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse_log_det_jacobian of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>], [True, True, True, True, False, False, False, False, False, False, False, False, False, False, False], PyTreeDef({'_batch_shape': (), '_bijector': CustomNode(Block[([None, None, None, 1, 1, True, True, 1], [True, True, True, False, False, False, False, False], PyTreeDef({'_bijector': CustomNode(ScalarAffine[([2, 0, 0, None, True, True, None, None, 0.0], [False, False, False, True, False, False, True, True, False], PyTreeDef({'_batch_shape': (*,), '_event_ndims_in': *, '_event_ndims_out': *, '_inv_scale': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_log_scale': *, '_scale': *, '_shift': *}))], [None, None, None, *, None, None, *, *, None]), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_ndims': *}))], [*, *, *, None, None, None, None, None]), '_diag': *, '_dtype': *, '_event_dims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, 'forward': *, 'forward_log_det_jacobian': *, 'inverse': *, 'inverse_and_log_det': *, 'inverse_log_det_jacobian': *}))], [*, *, *, *, None, None, None, None, None, None, None, None, None, None, None])], '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *}))], [*, *, *, *, *, None, None, None, None]), '_distribution': CustomNode(Independent[([None, None, 1], [True, True, False], PyTreeDef({'_distribution': CustomNode(Normal[([None, None], [True, True], PyTreeDef({'_loc': *, '_scale': *}))], [*, *]), '_reinterpreted_batch_ndims': *}))], [*, *, None]), '_dtype': *, '_event_shape': (*,), '_loc': *, '_scale': CustomNode(DiagLinear[([None, None, None, None, dtype('float32'), 2, 1, 1, True, True, <bound method Block.forward of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.forward_log_det_jacobian of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse_and_log_det of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>, <bound method Block.inverse_log_det_jacobian of <distrax._src.bijectors.block.Block object at 0x18a6fd750>>], [True, True, True, True, False, False, False, False, False, False, False, False, False, False, False], PyTreeDef({'_batch_shape': (), '_bijector': CustomNode(Block[([None, None, None, 1, 1, True, True, 1], [True, True, True, False, False, False, False, False], PyTreeDef({'_bijector': CustomNode(ScalarAffine[([2, 0, 0, None, True, True, None, None, 0.0], [False, False, False, True, False, False, True, True, False], PyTreeDef({'_batch_shape': (*,), '_event_ndims_in': *, '_event_ndims_out': *, '_inv_scale': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_log_scale': *, '_scale': *, '_shift': *}))], [None, None, None, *, None, None, *, *, None]), '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, '_ndims': *}))], [*, *, *, None, None, None, None, None]), '_diag': *, '_dtype': *, '_event_dims': *, '_event_ndims_in': *, '_event_ndims_out': *, '_is_constant_jacobian': *, '_is_constant_log_det': *, 'forward': *, 'forward_log_det_jacobian': *, 'inverse': *, 'inverse_and_log_det': *, 'inverse_log_det_jacobian': *}))], [*, *, *, *, None, None, None, None, None, None, None, None, None, None, None]), '_scale_diag': *}))], [*, *, *, *, *, *, *, None, None, *, *, *, *, *, *]), '_dtype': None, '_event_shape': None})); value: <distrax._src.distributions.transformed.Transformed object at 0x18a8839d0>."
     ]
    }
   ],
   "source": [
    "key, subkey = jr.split(key)\n",
    "\n",
    "# Train on the unbounded space\n",
    "flow, losses = fit_to_data(\n",
    "    key=subkey,\n",
    "    nsf_constructor=nsf_constructor,\n",
    "    x=x,\n",
    "    learning_rate=5e-4,\n",
    "    max_epochs=70,\n",
    "    return_best=False,\n",
    "    batch_size=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763fc7cd-afe8-49ac-914b-9be09aeb53cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
